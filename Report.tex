\documentclass{article}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage[margin=1in]{geometry}
\usepackage{float}  
\usepackage{adjustbox}  
\usepackage{amsmath}
\usepackage{cite}

\begin{document}

\title{Generated Model Performance Report}
\author{Simon Green}
\date{\today}
\maketitle

\section{Results and Model Comparison} 

This report presents the performance evaluation of four reinforcement learning models: TRPO, PPO, TRPOER, and TRPOR. These models are implemented using \textbf{Stable Baselines3} and utilize \textbf{mini-batch gradient descent} for optimization, ensuring efficient and stable updates during training. The entropy calculations guiding the models are based on the entropy of each batch, which influences regularization and experience replay mechanisms. 


\subsection{TRPO (Trust Region Policy Optimization)}

Originally proposed by Schulman et al. \cite{schulman2017trustregionpolicyoptimization}, TRPO is a policy gradient method that constrains updates using a trust region to ensure stability in training.

\subsection{PPO (Proximal Policy Optimization)}

Introduced by Schulman et al. \cite{schulman2017proximalpolicyoptimizationalgorithms}, PPO improves upon TRPO by using a clipped surrogate objective to ensure efficient and stable policy updates.

\subsection{TRPOR (TRPO with Entropy Regularization)}

This model extends TRPO by introducing entropy regularization only in the policy objective. The entropy coefficient hyperparameter guides the degree of regularization, ensuring a balance between exploration and exploitation. The entropy guiding this model is computed at the batch level, dynamically adjusting policy updates.

\subsection{TRPOER (TRPO with Entropy Regularized Experience Replay)}

This model extends TRPO by incorporating entropy-based experience replay and an additional policy entropy regularization term. It utilizes a prioritized experience replay buffer sampled according to batch entropy values and a hyperparameter coefficient. The method enables bidirectional adaptive sampling, adjusting both the number and direction of sampled experiences to optimize learning. The adaptive sampling function is formulated as:

\begin{equation}
    S = \text{clip} \left( \left( M - m \right) \times 
    \begin{cases}
        1 - \left| \frac{H}{| \lambda + \epsilon |} \right|, & \lambda > 0 \\
        \left| \frac{H}{| \lambda + \epsilon |} \right|, & \lambda < 0
    \end{cases}
    + m, \; m, \; M \right)
\end{equation}

where \( S \) is the number of samples, \( H \) represents batch entropy, \( \lambda \) is the sampling coefficient, and \( M, m \) are the maximum and minimum sample limits.

%----------------------------------------------------------

\subsection{GenTRPO (Generative Experience Replay Trust Region Policy Optimization with Entropy Regularization)}

Quite a mouth full, we'll find a better name. The GenTRPO algorithm extends the Trust Region Policy Optimization with Entropy Regularization (TRPOER) framework \cite{schulman2017proximalpolicyoptimizationalgorithms,schulman2017trustregionpolicyoptimization} by incorporating a generative model to augment the experience replay buffer. The key idea is to leverage synthetic experiences generated by a forward dynamics model to complement real experiences, thereby improving exploration and sample efficiency. 

In the GenTRPO framework, the experiences used for policy updates are sampled from a replay buffer. The sampling strategy ensures that half of the samples in each batch are real experiences collected from the environment, while the other half are generated by the forward dynamics model. This combination of real and synthetic data balances model fidelity with exploratory richness, enabling the policy to generalize effectively while maintaining stability during optimization.

The generative component of GenTRPO relies on a forward dynamics model inspired by the intrinsic curiosity module \cite{pathak2017curiositydrivenexplorationselfsupervisedprediction}. The forward dynamics model comprises an encoder and a dynamics predictor. The encoder maps raw states \( s \) into a compact latent space representation \( h(s) \), capturing the essential features of the environment. The dynamics predictor then takes the latent state \( h(s) \) and action \( a \) as input and predicts the next latent state \( h(s') \), effectively modeling the transition function \( P(s' | s, a) \). The error of this model, expressed as 

\begin{equation}
  \mathcal{F}(s, a, s', r) = \frac{1}{2} \left\| g(h(s), a) - h(s') \right\|^2
\end{equation}

where \( g(h(s), a) \) is the predicted latent state, \( h(s') \) is the true latent state, and \( \| \cdot \| \) represents the Euclidean norm. This error quantifies how accurately the forward dynamics model predicts the latent state transitions. It is used to compute intrinsic motivation, encouraging the agent to explore transitions that are harder to predict, thereby fostering exploration  \cite{wang2024prioritizedgenerativereplay}.


%----------------------------------------------------------

\section{Model Performance Table}

The table below summarizes the models' performance in terms of mean and standard deviation of rewards, along with maximum and minimum rewards recorded during training. A higher mean reward suggests better overall performance, while lower standard deviation indicates increased stability.

\bigskip
\begin{center}
  \input{.assets/model_comparison.tex}
\end{center}
\bigskip

\section{Performance Analysis Through Plots}

The following plots visualize different aspects of model performance.

\subsection*{Resampled Rewards and Outlier Removal}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{.assets/resampled_outlier.png}
    \caption{Resampled Rewards with Outlier Removal}
\end{figure}

This plot presents reward distributions after applying smoothing and outlier removal techniques, filtering out misleading fluctuations.

\subsection*{Learning Stability}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{.assets/learning_stability.png}
    \caption{Learning Stability for Different Models}
\end{figure}

Learning stability is evaluated based on the smoothness of the reward curve. A more stable learning process exhibits a steadily increasing reward trajectory, whereas high variance suggests instability due to sensitivity to hyperparameters.

\subsection*{Learning Stability (Coefficient of Variation)}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{.assets/learning_stability_cv.png}
    \caption{Learning Stability (Coefficient of Variation)}
\end{figure}

The coefficient of variation (CV) provides a normalized measure of stability. A lower CV signifies less volatile performance, whereas a higher CV indicates inconsistency due to randomness in training.

\subsection*{Sample Efficiency}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{.assets/sample_efficiency.png}
    \caption{Sample Efficiency Across Models}
\end{figure}

Sample efficiency measures how quickly a model improves with limited training episodes. Higher sample efficiency is desirable, especially in data-scarce scenarios.

\subsection*{Combined Sample Efficiency Results}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{.assets/sample_efficiency_combined.png}
    \caption{Combined Sample Efficiency Results}
\end{figure}

The combined sample efficiency plot aggregates results across all environments, showing how different models perform in terms of data efficiency.

\subsection*{Raw Data}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{.assets/raw_data.png}
    \caption{Raw Reward Data for Different Models}
\end{figure}

The raw data plot displays the recorded reward values without any smoothing. It provides insights into the actual training process and variability in rewards.

\newpage

\bibliographystyle{plain}
\bibliography{references}

\end{document}
