# Tuned
Pendulum-v1:
  policy: 'MlpPolicy'
  n_timesteps: 100000
  reward_threshold: -3.0
  n_steps: 1024
  gamma: 0.9
  n_critic_updates: 15
  use_sde: True
  sde_sample_freq: 4
  n_envs: 2

# Tuned
LunarLanderContinuous-v3:
  policy: 'MlpPolicy'
  n_timesteps: 100000
  reward_threshold: 195
  batch_size: 32
  n_steps: 2048
  gamma: 0.995
  learning_rate: 0.0014
  n_critic_updates: 20
  cg_max_steps: 25
  target_kl: 0.01
  gae_lambda: 0.92
  net_arch: medium
  activation_fn: relu
  n_envs: 8

# Tuned
Ant-v5:
  policy: 'MlpPolicy'
  batch_size: 512
  n_steps: 2048
  gamma: 0.99
  learning_rate: 0.0013931381527875178
  line_search_shrinking_factor: 0.9
  n_critic_updates: 20
  cg_max_steps: 30
  cg_damping: 0.01
  target_kl: 0.05
  gae_lambda: 0.8
  net_arch: medium
  log_std_init: -2.32457966159704
  sde_sample_freq: 256
  ortho_init: 1
  activation_fn: relu
  lr_schedule: linear
  n_timesteps: 600000
  n_envs: 6

# Tuned
Humanoid-v5:
  policy: 'MlpPolicy'
  n_timesteps: 100000
  batch_size: 256
  n_steps: 32
  gamma: 0.999
  learning_rate: 0.07439215571634901
  n_critic_updates: 20
  cg_max_steps: 5
  target_kl: 0.02
  gae_lambda: 0.92
  net_arch: small
  activation_fn: tanh
  reward_threshold: 450
  n_envs: 8

# Tuned
InvertedDoublePendulum-v5:
  policy: 'MlpPolicy'
  batch_size: 512
  n_steps: 128
  gamma: 0.995
  learning_rate: 0.000641801789987054
  line_search_shrinking_factor: 0.8
  n_critic_updates: 20
  cg_max_steps: 20
  cg_damping: 0.2
  target_kl: 0.005
  gae_lambda: 0.8
  net_arch: medium
  log_std_init: -1.1442557317275153
  sde_sample_freq: 64
  ortho_init: 0
  activation_fn: relu
  lr_schedule: constant
  n_timesteps: 200000
  n_envs: 8

# Tuned
RocketLander-v0:
  policy: 'MlpPolicy'
  n_timesteps: 100000
  reward_threshold: -3.0
  batch_size: 32
  n_steps: 1024
  gamma: 0.999
  learning_rate: 1.1367854351821837e-05
  n_critic_updates: 25
  cg_max_steps: 20
  target_kl: 0.05
  gae_lambda: 0.92
  net_arch: small
  activation_fn: relu

